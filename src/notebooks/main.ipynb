{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13357,"status":"ok","timestamp":1713299646562,"user":{"displayName":"Karol Charyło","userId":"05925906670472823206"},"user_tz":-120},"id":"RLPoiRHhE4Mp","outputId":"d363179b-ee59-4c98-8153-b84a3757710e"},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime\n","import time\n","import json\n","from IPython.display import clear_output\n","from pathlib import Path\n","\n","import torch\n","import torch.nn as nn\n","from torchvision import datasets\n","from torchvision.transforms import v2\n","from torchvision.models import resnet50, ResNet50_Weights\n","\n","from skimage.feature import local_binary_pattern\n","from skimage.color import rgb2gray\n","from scipy.ndimage import gaussian_filter\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1713299650636,"user":{"displayName":"Karol Charyło","userId":"05925906670472823206"},"user_tz":-120},"id":"6GNUfoNwFm28"},"outputs":[],"source":["import sys\n","sys.path.append('/zhome/ac/d/174101/thesis/src')\n","from utils.ResNet import ResNet\n","from utils.Hyperparams import Hyperparams\n","from utils.train import  train, evaluate"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1713299650637,"user":{"displayName":"Karol Charyło","userId":"05925906670472823206"},"user_tz":-120},"id":"dNwhPVrWG-kr"},"outputs":[],"source":["img_path = '/work3/s220243/Thesis'\n","base_path = '/zhome/ac/d/174101/thesis'\n","df = pd.read_excel(f\"{base_path}/data/imageAnalysis_information.xlsx\")\n","df  = pd.DataFrame(df.values[1:], columns=df.iloc[0])\n","df['species'] = df['species'].str.strip()\n","df['genus'] = df['genus'].str.strip()\n","species_genus_df = pd.DataFrame({\"IBT_number\": df['IBT number'],\n","                          \"Target\" : df[\"genus\"]+\"-\"+df[\"species\"]})"]},{"cell_type":"code","execution_count":94,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1713299650637,"user":{"displayName":"Karol Charyło","userId":"05925906670472823206"},"user_tz":-120},"id":"HACiEQ9nFJqN"},"outputs":[],"source":["# resnetX = (Num of channels, repetition, Bottleneck_expansion , Bottleneck_layer)\n","model_parameters={}\n","model_parameters['resnet18'] = ([64,128,256,512],[2,2,2,2],1,False)\n","model_parameters['resnet34'] = ([64,128,256,512],[3,4,6,3],1,False)\n","model_parameters['resnet50'] = ([64,128,256,512],[3,4,6,3],4,True)\n","model_parameters['resnet101'] = ([64,128,256,512],[3,4,23,3],4,True)\n","model_parameters['resnet152'] = ([64,128,256,512],[3,8,36,3],4,True)"]},{"cell_type":"code","execution_count":95,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1713299650637,"user":{"displayName":"Karol Charyło","userId":"05925906670472823206"},"user_tz":-120},"id":"44DlMP_UspqI"},"outputs":[{"data":{"text/plain":["\"\\nclass LBP:\\n    def __init__(self, radius=1, n_points=8):\\n        self.radius = radius\\n        self.n_points = n_points\\n\\n    def __call__(self, image):\\n        image_gray = rgb2gray(image)\\n        image_gaussian = gaussian_filter(image_gray, sigma=0.5)\\n        lbp_image = local_binary_pattern(image_gaussian, self.n_points, self.radius, method='uniform')\\n\\n        pseudo_rgb_image = np.stack([lbp_image] * 3, axis=0)\\n\\n        return pseudo_rgb_image\\n\""]},"execution_count":95,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","class LBP:\n","    def __init__(self, radius=1, n_points=8):\n","        self.radius = radius\n","        self.n_points = n_points\n","\n","    def __call__(self, image):\n","        image_gray = rgb2gray(image)\n","        image_gaussian = gaussian_filter(image_gray, sigma=0.5)\n","        lbp_image = local_binary_pattern(image_gaussian, self.n_points, self.radius, method='uniform')\n","\n","        pseudo_rgb_image = np.stack([lbp_image] * 3, axis=0)\n","\n","        return pseudo_rgb_image\n","'''"]},{"cell_type":"code","execution_count":132,"metadata":{},"outputs":[],"source":["from PIL import Image\n","\n","class LBP:\n","    def __init__(self, radius=1, n_points=8):\n","        self.radius = radius\n","        self.n_points = n_points\n","\n","    def __call__(self, image):\n","        image_gray = rgb2gray(image)\n","        image_gaussian = gaussian_filter(image_gray, sigma=0.5)\n","        lbp_image = local_binary_pattern(image_gaussian, self.n_points, self.radius, method='uniform')\n","\n","        x = np.stack([lbp_image] * 3, axis=-1)\n","        pseudo_rgb_image = (x-np.min(x))/(np.max(x)-np.min(x))\n","        pseudo_rgb_image = (pseudo_rgb_image * 255).astype(np.uint8)  # Convert to uint8 for PIL compatibility\n","        pseudo_rgb_image = Image.fromarray(pseudo_rgb_image)\n","\n","        return pseudo_rgb_image"]},{"cell_type":"code","execution_count":97,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1713299650638,"user":{"displayName":"Karol Charyło","userId":"05925906670472823206"},"user_tz":-120},"id":"mgE6U5UYUAfk"},"outputs":[],"source":["def load_model_ResNet(architecture):\n","  if architecture == 'pretrained':\n","    model = resnet50(weights=ResNet50_Weights.DEFAULT)\n","    model.fc = nn.Linear(1000, num_classes)\n","    #Freeze all but final layer\n","    for name, param in model.named_parameters():\n","      if \"fc\" in name:\n","        param.requires_grad = True\n","      else:\n","        param.requires_grad = False\n","  else:\n","    model = ResNet(model_parameters[architecture] , in_channels=3, num_classes=num_classes)\n","  return model"]},{"cell_type":"code","execution_count":98,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1713299650638,"user":{"displayName":"Karol Charyło","userId":"05925906670472823206"},"user_tz":-120},"id":"8t6SKgk81Mas"},"outputs":[],"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"code","execution_count":133,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1713299650638,"user":{"displayName":"Karol Charyło","userId":"05925906670472823206"},"user_tz":-120},"id":"2H3IiTmXsMds"},"outputs":[],"source":["data_transforms = {\n","    'train': v2.Compose([\n","        LBP(radius=1, n_points=8),\n","        v2.Resize((224,224)),\n","        #v2.ToTensor(),\n","        v2.PILToTensor(),\n","        v2.ToDtype(torch.float32, scale=True),\n","        #v2.PILToTensor(),\n","        v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'test': v2.Compose([\n","        LBP(radius=1, n_points=8),\n","        v2.Resize(224),\n","        v2.ToTensor(),\n","        ##v2.ToDtype(torch.float32, scale=True),\n","        v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'validation': v2.Compose([\n","        LBP(radius=1, n_points=8),\n","        v2.Resize(224),\n","        v2.ToTensor(),\n","        ##v2.ToDtype(torch.float32, scale=True),\n","        v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n","# Local Binary Patters - take a look\n","# Fourier Transform - take a look"]},{"cell_type":"code","execution_count":100,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":522,"status":"ok","timestamp":1713299651151,"user":{"displayName":"Karol Charyło","userId":"05925906670472823206"},"user_tz":-120},"id":"zY89EXOc9F7J","outputId":"ae6e7083-7a15-4370-aaaa-7271a460e830"},"outputs":[{"name":"stdout","output_type":"stream","text":["Enabling notebook extension jupyter-js-widgets/extension...\n","      - Validating: \u001b[32mOK\u001b[0m\n"]}],"source":["! jupyter nbextension enable --py widgetsnbextension"]},{"cell_type":"code","execution_count":101,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1713299651151,"user":{"displayName":"Karol Charyło","userId":"05925906670472823206"},"user_tz":-120},"id":"liyS61YEV9qw","outputId":"f2a05ec1-7c82-4954-d345-a52677709cc9"},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["# set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":102,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1713299651151,"user":{"displayName":"Karol Charyło","userId":"05925906670472823206"},"user_tz":-120},"id":"yoW-KzX3IgAZ"},"outputs":[],"source":["# Define the architecture\n","architecture = 'resnet50'\n","num_classes = len(species_genus_df['Target'])\n","model = load_model_ResNet(architecture) #use architecture for training a full model\n","\n","# Move the model to the GPU if available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)"]},{"cell_type":"code","execution_count":103,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1713299651152,"user":{"displayName":"Karol Charyło","userId":"05925906670472823206"},"user_tz":-120},"id":"HdprBob9yaz2"},"outputs":[],"source":["base = Path(f\"{img_path}\")\n","checkpoint_dst = base / \"checkpoints\"\n","checkpoint_dst.mkdir(exist_ok=True)\n","checkpoint_path = checkpoint_dst / \"checkpoint.pth\""]},{"cell_type":"code","execution_count":104,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1713299651152,"user":{"displayName":"Karol Charyło","userId":"05925906670472823206"},"user_tz":-120},"id":"gpuDP8jm9K9Q"},"outputs":[],"source":["#create DataFrame\n","trained_models_df = pd.DataFrame(columns=[\n","    'filename',\n","    'train_start_datetime',\n","    'batch_size',\n","    'lr',\n","    'layers',\n","    'optimizer',\n","    'loss',\n","    'trained_epochs',\n","    'last_saved_epoch',\n","    'best_val_loss',\n","    'early_stopping',\n","    'epoch_train_losses',\n","    'epoch_val_losses',\n","    'epoch_train_times',\n","    'total_train_time',\n","])\n","\n","model_dst = base / \"models\"\n","model_dst.mkdir(exist_ok=True)\n","TRAINED_MODELS_CSV = base / \"models/trained_models.csv\"\n","\n","isExist = os.path.exists(TRAINED_MODELS_CSV)\n","if not isExist:\n","    trained_models_df.to_csv(TRAINED_MODELS_CSV)\n","    print(\"Created trained_models.csv\")"]},{"cell_type":"code","execution_count":105,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1713299651152,"user":{"displayName":"Karol Charyło","userId":"05925906670472823206"},"user_tz":-120},"id":"UaK-Npib0vPI"},"outputs":[],"source":["hyperparams = Hyperparams(Path(base_path) / \"data/train_conf.toml\")"]},{"cell_type":"code","execution_count":106,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1713299651153,"user":{"displayName":"Karol Charyło","userId":"05925906670472823206"},"user_tz":-120},"id":"JivdHaljzHO9","outputId":"a683b7c8-769c-4710-8b26-b61446ba0933"},"outputs":[{"data":{"text/plain":["'2024-04-22 18:38:25.093099'"]},"execution_count":106,"metadata":{},"output_type":"execute_result"}],"source":["train_start_datetime = datetime.now()\n","str(train_start_datetime)"]},{"cell_type":"code","execution_count":107,"metadata":{"executionInfo":{"elapsed":568,"status":"ok","timestamp":1713299651714,"user":{"displayName":"Karol Charyło","userId":"05925906670472823206"},"user_tz":-120},"id":"Wmwr4F1d0zuI"},"outputs":[],"source":["num_epochs = hyperparams.epochs\n","#batch_size = hyperparams.batch_size\n","batch_size = 8\n","lr = hyperparams.lr\n","early_stopping_patience = 10\n","optimizer = hyperparams.optimizer_class(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n","loss_fn = hyperparams.loss_fn"]},{"cell_type":"code","execution_count":108,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1713299651717,"user":{"displayName":"Karol Charyło","userId":"05925906670472823206"},"user_tz":-120},"id":"rnbulaFj0iEf"},"outputs":[],"source":["metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0]\n","jaccard_epoch = []\n","f1_epoch = []\n","recall_epoch = []\n","precision_epoch = []\n","acc_epoch = []"]},{"cell_type":"code","execution_count":134,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1232,"status":"ok","timestamp":1713299652943,"user":{"displayName":"Karol Charyło","userId":"05925906670472823206"},"user_tz":-120},"id":"WebNq_A_DAEP","outputId":"e01cd537-e682-4684-d361-4c867ea1c026"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'train': 6901, 'test': 3193}\n"]}],"source":["# Define the data directory\n","data_dir = Path(img_path) / 'data_split'\n","\n","# Create data loaders\n","image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'test']}\n","\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True) for x in ['train', 'test']}\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n","print(dataset_sizes)\n","\n","class_names = image_datasets['train'].classes\n","#class_names"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yXqYenWd0MsP","outputId":"fea36e4d-623b-49e8-df8f-aa31d7780ba7"},"outputs":[{"name":"stderr","output_type":"stream","text":["Training:   0%|          | 0/863 [00:00<?, ?it/s]"]},{"ename":"TypeError","evalue":"Input tensor should be a float tensor. Got torch.uint8.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[88], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     19\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 21\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train(model, dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], optimizer, loss_fn, device)\n\u001b[1;32m     22\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m evaluate(model, dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m], loss_fn, device)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Saving the model \"\"\"\u001b[39;00m\n","File \u001b[0;32m~/thesis/src/train.py:18\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#Progress bar\u001b[39;00m\n\u001b[1;32m     16\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(loader), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(loader),leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m#y = y.to(device, dtype=torch.float32)\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/fungi/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m~/miniconda3/envs/fungi/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m~/miniconda3/envs/fungi/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m~/miniconda3/envs/fungi/lib/python3.12/site-packages/torchvision/datasets/folder.py:231\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n","File \u001b[0;32m~/miniconda3/envs/fungi/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/fungi/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/fungi/lib/python3.12/site-packages/torchvision/transforms/v2/_container.py:51\u001b[0m, in \u001b[0;36mCompose.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     49\u001b[0m needs_unpacking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 51\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m transform(\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m     52\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m outputs \u001b[38;5;28;01mif\u001b[39;00m needs_unpacking \u001b[38;5;28;01melse\u001b[39;00m (outputs,)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m~/miniconda3/envs/fungi/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/fungi/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/fungi/lib/python3.12/site-packages/torchvision/transforms/v2/_transform.py:51\u001b[0m, in \u001b[0;36mTransform.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     45\u001b[0m needs_transform_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[1;32m     46\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_params(\n\u001b[1;32m     47\u001b[0m     [inpt \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[38;5;28;01mif\u001b[39;00m needs_transform]\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m flat_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(inpt, params) \u001b[38;5;28;01mif\u001b[39;00m needs_transform \u001b[38;5;28;01melse\u001b[39;00m inpt\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n","File \u001b[0;32m~/miniconda3/envs/fungi/lib/python3.12/site-packages/torchvision/transforms/v2/_misc.py:165\u001b[0m, in \u001b[0;36mNormalize._transform\u001b[0;34m(self, inpt, params)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, inpt: Any, params: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_kernel(F\u001b[38;5;241m.\u001b[39mnormalize, inpt, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean, std\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n","File \u001b[0;32m~/miniconda3/envs/fungi/lib/python3.12/site-packages/torchvision/transforms/v2/_transform.py:35\u001b[0m, in \u001b[0;36mTransform._call_kernel\u001b[0;34m(self, functional, inpt, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_kernel\u001b[39m(\u001b[38;5;28mself\u001b[39m, functional: Callable, inpt: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     34\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m _get_kernel(functional, \u001b[38;5;28mtype\u001b[39m(inpt), allow_passthrough\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m kernel(inpt, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/fungi/lib/python3.12/site-packages/torchvision/transforms/v2/functional/_misc.py:37\u001b[0m, in \u001b[0;36mnormalize_image\u001b[0;34m(image, mean, std, inplace)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;129m@_register_kernel_internal\u001b[39m(normalize, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;129m@_register_kernel_internal\u001b[39m(normalize, tv_tensors\u001b[38;5;241m.\u001b[39mImage)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalize_image\u001b[39m(image: torch\u001b[38;5;241m.\u001b[39mTensor, mean: List[\u001b[38;5;28mfloat\u001b[39m], std: List[\u001b[38;5;28mfloat\u001b[39m], inplace: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m image\u001b[38;5;241m.\u001b[39mis_floating_point():\n\u001b[0;32m---> 37\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput tensor should be a float tensor. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected tensor to be a tensor image of size (..., C, H, W). Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mTypeError\u001b[0m: Input tensor should be a float tensor. Got torch.uint8."]}],"source":["# Training the model\n","\n","train_start_datetime = datetime.now()\n","\n","train_losses = []\n","test_losses = []\n","lr_epochs = []\n","\n","epoch_times = []\n","\n","trained_epochs = 0\n","last_saved_epoch = 0\n","early_stopping = False\n","\n","best_test_loss = float(\"inf\")\n","\n","train_start = time.time()\n","for epoch in range(num_epochs):\n","    start_time = time.time()\n","\n","    train_loss = train(model, dataloaders['train'], optimizer, loss_fn, device)\n","    test_loss = evaluate(model, dataloaders['test'], loss_fn, device)\n","\n","    \"\"\" Saving the model \"\"\"\n","    if test_loss < best_test_loss:\n","        torch.save(model.state_dict(), checkpoint_path)\n","\n","    if epoch % 5 == 4:\n","        clear_output(wait=True)\n","\n","    if test_loss < best_test_loss:\n","        data_str = f\"Valid loss improved from {best_test_loss:2.4f} to {test_loss:2.4f}. Saving checkpoint: {checkpoint_path}\"\n","        print(data_str)\n","\n","        best_test_loss = test_loss\n","        last_saved_epoch = epoch+1\n","\n","    end_time = time.time()\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    trained_epochs = epoch+1\n","    data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\\n'\n","    data_str += f'\\tTrain Loss: {train_loss:.3f}\\n'\n","    data_str += f'\\t Test. Loss: {test_loss:.3f}\\n'\n","    print(data_str)\n","\n","    train_losses.append(train_loss)\n","    test_losses.append(test_loss)\n","    epoch_times.append(end_time - start_time)\n","    lr_epochs.append(lr)\n","\n","\n","    if trained_epochs - last_saved_epoch >= early_stopping_patience:\n","        early_stopping = True\n","        break\n","\n","train_end = time.time()\n","train_mins, train_secs = epoch_time(train_start, train_end)\n","data_str = f'Training completed:\\n'\n","data_str += f'\\tEpochs: \\t{trained_epochs}\\n'\n","data_str += f'\\tTraining Time: \\t{train_mins}m {train_secs}s\\n'\n","# data_str += f'\\tTrain Loss: \\t{train_loss:.3f}\\n'\n","# data_str += f'\\t Test. Loss: \\t{test_loss:.3f}\\n'\n","print(data_str)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q4Nma-RPfZ7D"},"outputs":[{"ename":"AttributeError","evalue":"'Hyperparams' object has no attribute 'layers'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dict_to_append \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m: [hyperparams\u001b[38;5;241m.\u001b[39mmodel_name()],\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_start_datetime\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28mstr\u001b[39m(train_start_datetime)],\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: [hyperparams\u001b[38;5;241m.\u001b[39mbatch_size],\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: [hyperparams\u001b[38;5;241m.\u001b[39mlr],\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m: [hyperparams\u001b[38;5;241m.\u001b[39moptimizer],\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: [hyperparams\u001b[38;5;241m.\u001b[39mloss],\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m: [trained_epochs],\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_saved_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m: [last_saved_epoch],\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_val_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: [best_test_loss],\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearly_stopping\u001b[39m\u001b[38;5;124m'\u001b[39m: [early_stopping],\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch_train_losses\u001b[39m\u001b[38;5;124m'\u001b[39m: [json\u001b[38;5;241m.\u001b[39mdumps(train_losses)],\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch_val_losses\u001b[39m\u001b[38;5;124m'\u001b[39m: [json\u001b[38;5;241m.\u001b[39mdumps(test_losses)],\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch_train_times\u001b[39m\u001b[38;5;124m'\u001b[39m: [json\u001b[38;5;241m.\u001b[39mdumps(epoch_times)],\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_train_time\u001b[39m\u001b[38;5;124m'\u001b[39m: [train_end \u001b[38;5;241m-\u001b[39m train_start],\n\u001b[1;32m     16\u001b[0m }\n","File \u001b[0;32m~/thesis/src/Hyperparams.py:23\u001b[0m, in \u001b[0;36mHyperparams.model_name\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_name\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     22\u001b[0m     formatted_lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.3e}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr)\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfungi_model_B\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_E\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_lr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformatted_lr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_L\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","\u001b[0;31mAttributeError\u001b[0m: 'Hyperparams' object has no attribute 'layers'"]}],"source":["dict_to_append = {\n","    'filename': [hyperparams.model_name()],\n","    'train_start_datetime': [str(train_start_datetime)],\n","    'batch_size': [hyperparams.batch_size],\n","    'lr': [hyperparams.lr],\n","    'optimizer': [hyperparams.optimizer],\n","    'loss': [hyperparams.loss],\n","    'trained_epochs': [trained_epochs],\n","    'last_saved_epoch': [last_saved_epoch],\n","    'best_val_loss': [best_test_loss],\n","    'early_stopping': [early_stopping],\n","    'epoch_train_losses': [json.dumps(train_losses)],\n","    'epoch_val_losses': [json.dumps(test_losses)],\n","    'epoch_train_times': [json.dumps(epoch_times)],\n","    'total_train_time': [train_end - train_start],\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6jw2uprI0fnw"},"outputs":[],"source":["new_row = pd.DataFrame.from_dict(dict_to_append)\n","# trained_models_df = pd.concat([trained_models_df, new_row], ignore_index=True)\n","new_row.to_csv(TRAINED_MODELS_CSV, mode='a', header=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2RXd5OUL-SWN"},"outputs":[{"ename":"AttributeError","evalue":"'Hyperparams' object has no attribute 'layers'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m base \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m hyperparams\u001b[38;5;241m.\u001b[39mmodel_name()\n\u001b[1;32m      3\u001b[0m isExist \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_save_path)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m isExist:\n","File \u001b[0;32m~/thesis/src/Hyperparams.py:23\u001b[0m, in \u001b[0;36mHyperparams.model_name\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_name\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     22\u001b[0m     formatted_lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.3e}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr)\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfungi_model_B\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_E\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_lr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformatted_lr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_L\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","\u001b[0;31mAttributeError\u001b[0m: 'Hyperparams' object has no attribute 'layers'"]}],"source":["model_save_path = base / \"models\" / hyperparams.model_name()\n","\n","isExist = os.path.exists(model_save_path)\n","if not isExist:\n","    torch.save(model.state_dict(), model_save_path)\n","    print(f\"Saved model to {model_save_path}\")\n","else:\n","    print(f\"WARNING: filename already exists, couldn't save {model_save_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iNDXRYTd-WIw"},"outputs":[],"source":["def plot_training(training_losses,\n","                  validation_losses,\n","                  gaussian=True,\n","                  sigma=2,\n","                  figsize=(8, 6)\n","                  ):\n","    \"\"\"\n","    Returns a loss plot with training loss, validation loss\n","    \"\"\"\n","\n","    import matplotlib.pyplot as plt\n","    from matplotlib import gridspec\n","    from scipy.ndimage import gaussian_filter\n","\n","    list_len = len(training_losses)\n","    x_range = list(range(1, list_len + 1))  # number of x values\n","\n","    fig = plt.figure(figsize=figsize)\n","    grid = gridspec.GridSpec(ncols=1, nrows=1, figure=fig)\n","\n","    subfig1 = fig.add_subplot(grid[0])\n","\n","    subfigures = fig.get_axes()\n","\n","    for i, subfig in enumerate(subfigures, start=1):\n","        subfig.spines['top'].set_visible(False)\n","        subfig.spines['right'].set_visible(False)\n","\n","    if gaussian:\n","        training_losses_gauss = gaussian_filter(training_losses, sigma=sigma)\n","        validation_losses_gauss = gaussian_filter(validation_losses, sigma=sigma)\n","\n","        linestyle_original = '.'\n","        color_original_train = 'lightcoral'\n","        color_original_valid = 'lightgreen'\n","        color_smooth_train = 'red'\n","        color_smooth_valid = 'green'\n","        alpha = 0.25\n","    else:\n","        linestyle_original = '-'\n","        color_original_train = 'red'\n","        color_original_valid = 'green'\n","        alpha = 1.0\n","\n","    # Subfig 1\n","    subfig1.plot(x_range, training_losses, linestyle_original, color=color_original_train, label='Training',\n","                 alpha=alpha)\n","    subfig1.plot(x_range, validation_losses, linestyle_original, color=color_original_valid, label='Validation',\n","                 alpha=alpha)\n","    if gaussian:\n","        subfig1.plot(x_range, training_losses_gauss, '-', color=color_smooth_train, label='Training', alpha=0.75)\n","        subfig1.plot(x_range, validation_losses_gauss, '-', color=color_smooth_valid, label='Validation', alpha=0.75)\n","    subfig1.title.set_text('Training & validation loss')\n","    subfig1.set_xlabel('Epoch')\n","    subfig1.set_ylabel('Loss')\n","\n","    subfig1.legend(loc='upper right')\n","\n","\n","    return fig"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dCz2QWM9-Xht"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","fig = plot_training(train_losses, test_losses, gaussian=True, sigma=1, figsize=(4,4))\n","plt.savefig(str(hyperparams.model_name())+'.jpg')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPEYW/W/3nJw+0MtrUPgGE4","gpuType":"V100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
